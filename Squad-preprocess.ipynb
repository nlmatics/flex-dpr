{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "import gzip\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "from transformers import DPRContextEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoder\n",
    "from transformers import DPRContextEncoder\n",
    "import csv\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast\n",
    "from torch.nn import CosineSimilarity\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from IPython import embed\n",
    "from sklearn.metrics import classification_report\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "# device = \"cpu\"\n",
    "torch.cuda.empty_cache()\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"4,5,6,7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmax(k, nums, blocked):\n",
    "    ix = []\n",
    "    while len(ix) < k:\n",
    "        w = max(nums)\n",
    "        w_ix = nums.tolist().index(w)\n",
    "        nums[w_ix] = float('-inf')\n",
    "        if w_ix != blocked:\n",
    "            ix.append(w_ix)\n",
    "    return ix\n",
    "\n",
    "def sample(j, nums, blocked):\n",
    "    rand = random.sample(list(range(len(nums))), 30)\n",
    "    true = []\n",
    "    for r in rand:\n",
    "        if r not in blocked:\n",
    "            true.append(r)\n",
    "    return true[0:20]\n",
    "    \n",
    "def generate_ixs(examples, file_out, ix_map=[]):\n",
    "    file = open(file_out, 'w+', newline ='\\n')\n",
    "    k_hard = 10\n",
    "    j_rand = 20\n",
    "    bm25 = BM25Okapi([word_tokenize(a) for a in examples['text']])\n",
    "    for i in tqdm(range(len(examples['question']))):\n",
    "        tokenized_query = examples['question'][i]\n",
    "        doc_scores = bm25.get_scores(word_tokenize(tokenized_query))\n",
    "        if ix_map:\n",
    "            block = ix_map[str(i)]\n",
    "        else:\n",
    "            block = i\n",
    "        hard_ix = kmax(k_hard, doc_scores, block)\n",
    "        rand_ix = sample(j_rand, doc_scores, hard_ix + [block])\n",
    "        passage_num = examples['map'][str(i)]\n",
    "        answers = [[passage_num] + hard_ix + rand_ix]\n",
    "        write = csv.writer(file)\n",
    "        write.writerows(answers)\n",
    "    file.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_name, file_out):\n",
    "    with open(file_name) as file: \n",
    "        data = json.load(file)\n",
    "    data = data['data']\n",
    "    example = {'question': [], 'text': [], 'map': {}, 'answers': [], 'title': {}}\n",
    "    question_count = 0\n",
    "    text_count = 0\n",
    "    for par in data:\n",
    "        title = \" \".join((par['title'].split(\"_\")))\n",
    "        par = par['paragraphs']\n",
    "        for elem in par:\n",
    "            for chunk in elem['qas']:\n",
    "                if chunk['is_impossible']: \n",
    "                    if len(chunk['plausible_answers']) == 0: continue\n",
    "                    example['question'].append(chunk['question'])\n",
    "                    #example['answers'].append(chunk['plausible_answers'][0])\n",
    "                else:\n",
    "                    example['question'].append(chunk['question'])\n",
    "                    #example['answers'].append(chunk['answers'][0])\n",
    "                example['map'][question_count] = text_count\n",
    "                question_count += 1\n",
    "            example['text'].append(title+ \" \" + elem['context'])\n",
    "            example['title'][text_count] = title \n",
    "            text_count += 1\n",
    "    with open(file_out, 'w+') as outfile:\n",
    "        json.dump(example, outfile)\n",
    "    return example\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPR(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, context_encoder,  question_encoder, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.question_encoder =  question_encoder\n",
    "        self.context_encoder = context_encoder#nn.DataParallel(context_encoder).to(device)\n",
    "        #self.context_head = torch.nn.Linear(1024, 2048)\n",
    "        #self.question_head = torch.nn.Linear(1024, 1024)\n",
    "        #self.tanh = torch.nn.Tanh()\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "        self.pretrained = pretrained\n",
    "\n",
    "    def forward(self, context_batch, question_batch):\n",
    "        #Context_batch (shape) : batchsize x seq_lenth : (questino_batch (shape))\n",
    "        cb = [torch.tensor(i).to(device) for i in context_batch]\n",
    "        #embeddings_context = self.context_encoder(*cb).last_hidden_state\n",
    "        if not self.pretrained:\n",
    "            embeddings_context = torch.stack([self.context_encoder(i).last_hidden_state[:,0,:] for i in cb])\n",
    "            embedding_question = self.question_encoder(torch.tensor(question_batch).to(device)).last_hidden_state[:,0,:]\n",
    "        else:\n",
    "            #enc = self.context_encoder\n",
    "            #embed()\n",
    "            embeddings_context = torch.stack([self.context_encoder(i)['pooler_output'].to(device) for i in cb]).to(device)\n",
    "            #embeddings_context = self.context_encoder(torch.tensor(context_batch).to(device))['pooler_output'].to(device)#torch.stack([self.context_encoder(i)['pooler_output'].to(device) for i in cb]).to(device)\n",
    "            embedding_question = self.question_encoder(torch.tensor(question_batch).to(device))['pooler_output'].to(device) \n",
    "        #embed()\n",
    "        logits = torch.einsum('ijk, ik ->ij', embeddings_context, embedding_question) \n",
    "        return logits\n",
    "\n",
    "    def predict(self,batch):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            question_batch = batch[0]\n",
    "            context_batch = batch[1]\n",
    "            context_ixs = batch[2]\n",
    "            labels = batch[3]\n",
    "\n",
    "            logits = self.log_softmax(self.forward(context_batch, question_batch))\n",
    "            loss = self.loss_fn(logits.to(device), torch.tensor(labels).to(device))\n",
    "            acc = sum([1 if logits.argmax(axis = -1)[i] == labels[i] else 0 for i in range(len(labels))]) / len(labels) \n",
    "\n",
    "        return logits.argmax(axis = -1), logits.max(axis=-1), loss, acc # predictions, loss\n",
    "\n",
    "\n",
    "    def criterion(self, batch):#context_batch, question_batch, labels):\n",
    "        question_batch = batch[0] \n",
    "        context_batch = batch[1]\n",
    "        context_ixs = batch[2]\n",
    "        labels = batch[3]\n",
    "\n",
    "        #batch_size, _ , = labels.shape\n",
    "        batch_size = len(context_batch)\n",
    "        #change this to BCELoss with logits \n",
    "        #loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "        logits = self.forward(context_batch, question_batch)\n",
    "\n",
    "        #loss = loss_func(logits.reshape(-1, 2), labels.reshape(-1).long())\n",
    "        loss = self.loss_fn(logits.to(device), torch.tensor(labels).to(device))\n",
    "        \n",
    "        logging_output = {\n",
    "            \"loss\": loss.item(), \n",
    "            \"accuracy\" : sum([1 if logits.argmax(axis = -1)[i] == labels[i] else 0 for i in range(len(labels))]) / batch_size   #(logits.argmax(axis = -1) == label).prod(axis = -1).sum() / batch_size\n",
    "        }\n",
    "        return loss, logging_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.weight', 'ctx_encoder.bert_model.pooler.dense.bias']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "ce = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "qe = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(device)\n",
    "\n",
    "def embed_phrases(data, token_max_len, model_file=None):\n",
    "    '''\n",
    "    Takes as input a list of strings (whether they be questions, contexts etc.) and tokenizes/embeds them\n",
    "    '''\n",
    "    if model_file is None:\n",
    "        with torch.no_grad():\n",
    "            context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "            context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "    else:\n",
    "        model = DPR(ce, qe)\n",
    "        l = torch.load(model_file)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in l.items():\n",
    "            mod = k.split('.')\n",
    "            if mod[1] != \"module\":\n",
    "                new_state_dict[k] = v\n",
    "                continue\n",
    "            del mod[1]\n",
    "            k = '.'.join(mod)\n",
    "            new_state_dict[k] = v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')#.to(device)\n",
    "        context_encoder = model.context_encoder\n",
    "    phrase_embeds = torch.empty((1, 769), dtype=torch.float).to(device)\n",
    "    for i in tqdm(range(len(data))):\n",
    "        passage_num = torch.tensor([[data[i][0]]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            tokenized = context_tokenizer(data[i][1], padding='max_length', max_length = token_max_len,truncation=True)\n",
    "            batch_embeds = context_encoder( torch.tensor([tokenized['input_ids']]).to(device) )[0]\n",
    "            final_val = torch.cat((passage_num, batch_embeds), 1).to(device)\n",
    "        phrase_embeds = torch.cat((phrase_embeds, final_val), 0).to(device)\n",
    "    return phrase_embeds[1:, :].cpu()\n",
    "\n",
    "def embed_contexts(data, model_file=None):\n",
    "    if model_file is None:\n",
    "        with torch.no_grad():\n",
    "            context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')#.to(device)\n",
    "            context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "    else:\n",
    "        model = DPR(ce, qe)\n",
    "        l = torch.load(model_file)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in l.items():\n",
    "            mod = k.split('.')\n",
    "            if mod[1] != \"module\":\n",
    "                new_state_dict[k] = v\n",
    "                continue\n",
    "            del mod[1]\n",
    "            k = '.'.join(mod)\n",
    "            new_state_dict[k] = v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')#.to(device)\n",
    "        context_encoder = model.context_encoder\n",
    "    phrase_embeds = torch.empty((1, 768), dtype=torch.float).to(device)\n",
    "    for i in tqdm(range(len(data))):\n",
    "        with torch.no_grad():\n",
    "            tokenized = context_tokenizer(data[i], padding='max_length', max_length = 512,truncation=True)\n",
    "            batch_embeds = context_encoder( torch.tensor([tokenized['input_ids']]).to(device) )[0]\n",
    "        phrase_embeds = torch.cat((phrase_embeds, batch_embeds), 0).to(device)\n",
    "    return phrase_embeds[1:, :].cpu()\n",
    "\n",
    "def embed_questions(data, model_file=None):\n",
    "    if model_file is None:\n",
    "        with torch.no_grad():\n",
    "            question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')#.to(device)\n",
    "            question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(device)\n",
    "    else:\n",
    "        model = DPR(ce, qe)\n",
    "        l = torch.load(model_file)\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in l.items():\n",
    "            mod = k.split('.')\n",
    "            if mod[1] != \"module\":\n",
    "                new_state_dict[k] = v\n",
    "                continue\n",
    "            del mod[1]\n",
    "            m = '.'.join(mod)\n",
    "            new_state_dict[m] = v\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')#.to(device)\n",
    "        question_encoder = model.question_encoder\n",
    "    phrase_embeds = torch.empty((1, 768), dtype=torch.float).to(device)\n",
    "    for i in tqdm(range(len(data))):\n",
    "        with torch.no_grad():\n",
    "            tokenized = question_tokenizer(data[i], padding='max_length', max_length = 512,truncation=True)\n",
    "            batch_embeds = question_encoder( torch.tensor([tokenized['input_ids']]).to(device) )[0]\n",
    "        phrase_embeds = torch.cat((phrase_embeds, batch_embeds), 0).to(device)\n",
    "    return phrase_embeds[1:, :].cpu()\n",
    "\n",
    "def phrase_creator(data, titles=[], phrase_len=[]):\n",
    "    punc = ['.', '!', '?']\n",
    "    phrases = []\n",
    "    for i in range(len(data)):\n",
    "        context_sents = data[i]\n",
    "        batch = \"\"\n",
    "        punc_count = 0\n",
    "        punc_diff = 0\n",
    "        for j in range(len(context_sents)):\n",
    "            if context_sents[j] in ['.', '!', '?']:\n",
    "                if punc_diff > 120: \n",
    "                    punc_count += 1\n",
    "                if punc_count >= phrase_len:\n",
    "                    punc_count = 0\n",
    "                    phrases.append((i, batch))\n",
    "                    batch = \"\"\n",
    "                punc_diff = 0\n",
    "                continue\n",
    "            batch = batch + context_sents[j]\n",
    "            punc_diff += 1\n",
    "        phrases.append((i, batch))\n",
    "    return phrases\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_vals(embeds, q_or_c, dev, phrase_len, data_set):\n",
    "    if q_or_c and dev:\n",
    "        torch.save(embeds, f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/{data_set}/dev-questions\")\n",
    "    elif q_or_c and not dev:\n",
    "        torch.save(embeds, f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/{data_set}/train-questions\")\n",
    "    elif not q_or_c and dev:\n",
    "        torch.save(embeds, f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/{data_set}/dev-contexts-{phrase_len}\")\n",
    "    else:\n",
    "        torch.save(embeds, f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/{data_set}/train-contexts-{phrase_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR SQUAD\n",
    "data_set = \"squad\"\n",
    "file_path = \"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/squad/dev-clean.json\"\n",
    "with open(file_path) as file: \n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR PUBMED\n",
    "data_set = \"pubmed-flex\"\n",
    "file_path = \"/home/ubuntu/nlm/noah/pubmed/small.json\"\n",
    "with open(file_path) as file: \n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR SCOTUS\n",
    "data_set = \"scotus-flex\"\n",
    "file_path = \"/home/ubuntu/nlm/noah/scotus/dev.json\"\n",
    "with open(file_path) as file: \n",
    "    data = json.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR NFCORPUS\n",
    "data_set = \"nfcorpus-flex\"\n",
    "file_path = \"/home/ubuntu/nlm/noah/nfcorpus/dev-clean.json\"\n",
    "with open(file_path) as file: \n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR P_RANK\n",
    "data_set = \"p_rank\"\n",
    "file_path = \"/home/ubuntu/nlm/noah/p_rank/dset.csv\"\n",
    "data = {\"text\": [], \"question\": [], \"map\": {}}\n",
    "first = True\n",
    "with open(file_path, newline='') as idx_file:\n",
    "    for line in csv.reader(idx_file):\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        row = list(line)\n",
    "        data['question'].append(row[1])\n",
    "        data['text'].append(row[2])\n",
    "        data['map'][int(row[0])] = int(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ubuntu/nlm/noah/squad/dev_raw.json\"\n",
    "file_out = \"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/squad/dev-clean.json\"\n",
    "data = preprocess(file_path, file_out)\n",
    "#phrases = phrase_creator(data['text'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:08<00:00, 14.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#embed and store questions\n",
    "embeds = embed_questions(data['question'], \"/home/ubuntu/nlm/noah/pubmed__meta.pt\")\n",
    "store_vals(embeds, True, True, 0, data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_encoder.question_encoder.bert_model.embeddings.position_ids\n",
      "question_encoder.question_encoder.bert_model.embeddings.word_embeddings.weight\n",
      "question_encoder.question_encoder.bert_model.embeddings.position_embeddings.weight\n",
      "question_encoder.question_encoder.bert_model.embeddings.token_type_embeddings.weight\n",
      "question_encoder.question_encoder.bert_model.embeddings.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.embeddings.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.self.query.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.self.query.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.self.key.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.self.key.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.self.value.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.self.value.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.output.dense.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.output.dense.bias\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "question_encoder.question_encoder.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.embeddings.position_ids\n",
      "context_encoder.ctx_encoder.bert_model.embeddings.word_embeddings.weight\n",
      "context_encoder.ctx_encoder.bert_model.embeddings.position_embeddings.weight\n",
      "context_encoder.ctx_encoder.bert_model.embeddings.token_type_embeddings.weight\n",
      "context_encoder.ctx_encoder.bert_model.embeddings.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.embeddings.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.self.query.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.self.query.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.self.key.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.self.key.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.self.value.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.self.value.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.output.dense.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.output.dense.bias\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "context_encoder.ctx_encoder.bert_model.encoder.layer.11.output.LayerNorm.bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "100%|██████████| 1000/1000 [01:08<00:00, 14.61it/s]\n"
     ]
    }
   ],
   "source": [
    "#embed and store contexts\n",
    "c_embeds = embed_contexts(data['text'], \"/home/ubuntu/nlm/noah/models/scotus_in_domain.pt\")\n",
    "store_vals(c_embeds, False, True, 0, data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "100%|██████████| 1500/1500 [01:59<00:00, 12.54it/s]\n"
     ]
    }
   ],
   "source": [
    "#embed phrases\n",
    "phrase_len = 5\n",
    "phrases = phrase_creator(data['text'], False, phrase_len)\n",
    "p_embeds = embed_phrases(phrases, 400, \"/home/ubuntu/nlm/noah/models/scotus_in_domain.pt\")\n",
    "store_vals(p_embeds, False, True, phrase_len, data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11858/11858 [01:16<00:00, 154.15it/s]\n"
     ]
    }
   ],
   "source": [
    "file_out = f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/{data_set}/dev_ixs.csv\"\n",
    "generate_ixs(data, file_out, data['map'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(266, \" For the State there are also advantages \\x97the more promptly imposed punishment after an admission of guilt may more effectively attain the objectives of punishment; and with the avoidance of trial, scarce judicial and prosecutorial resources are conserved for those cases in which there is a substantial issue of the defendant's guilt or in which there is substantial doubt that the State can sustain its burden of proof\")\n"
     ]
    }
   ],
   "source": [
    "print(phrases[28000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, context_batch, question_batch):\n",
    "        #Context_batch (shape) : batchsize x seq_lenth : (questino_batch (shape))\n",
    "        batch_size = 20\n",
    "        cb = [torch.tensor(i).to(device) for i in context_batch]\n",
    "        embeddings_context = []\n",
    "        #embeddings_context = self.context_encoder(*cb).last_hidden_state\n",
    "        for i in range(0, len(cb), batch_size):\n",
    "            end = min(len(cb), i+batch_size)\n",
    "            c_batch = cb[i:end]\n",
    "            if not self.pretrained:\n",
    "                embeddings_context = embeddings_context + [a.last_hidden_state[:,0,:] for a in self.context_encoder(c_batch)]\n",
    "            else:\n",
    "                embeddings_context = embeddings_context + [a['pooler_output'] for a in self.context_encoder(c_batch)]\n",
    "        embeddings_context = torch.stack(embeddings_context)\n",
    "        if not self.pretrained:\n",
    "            embedding_question = self.question_encoder(torch.tensor(question_batch).to(device)).last_hidden_state[:,0,:]\n",
    "        else:\n",
    "            embedding_question = self.question_encoder(torch.tensor(question_batch).to(device))['pooler_output']\n",
    "        logits = torch.einsum('ijk, ik ->ij', embeddings_context, embedding_question) \n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
