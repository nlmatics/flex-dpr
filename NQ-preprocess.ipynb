{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d111fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "import random\n",
    "import csv\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "from transformers import DPRContextEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoder\n",
    "from transformers import DPRContextEncoder\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "print(device)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45fb06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the indices of the k max elements,\n",
    "def kmax(k, nums, blocked):\n",
    "    ix = []\n",
    "    while len(ix) < k:\n",
    "        w_ix = np.argmax(nums, axis=0)\n",
    "        nums[w_ix] = float('-inf')\n",
    "        if w_ix != blocked:\n",
    "            ix.append(w_ix)\n",
    "    return ix\n",
    "\n",
    "def sample(j, nums, blocked):\n",
    "    rand = random.sample(list(range(len(nums))), 40)\n",
    "    for ind in rand:\n",
    "        if ind in blocked:\n",
    "            rand.remove(ind)\n",
    "    return rand[0:j]\n",
    "\n",
    "def NQ_file_open(file_object, device):\n",
    "    while True:\n",
    "        try:\n",
    "            chunk = json.loads(file_object.readline())\n",
    "            yield chunk\n",
    "        except:\n",
    "            yield \"NONE\"\n",
    "\n",
    "def NQ_index_processing(f, file_out, device):\n",
    "    examples = {'docs': [], 'questions': []}\n",
    "    count = 0\n",
    "    cap = 13000 #number of docs per question set in a dataset\n",
    "    with gzip.open(f, 'r') as file:\n",
    "        for chunk in NQ_file_open(file, device):\n",
    "            if count > cap: \n",
    "                break\n",
    "            if chunk == \"NONE\":\n",
    "                break\n",
    "            examples['questions'].append(chunk['question_text'].split(\" \"))\n",
    "            doc_string = []\n",
    "            for elem in chunk['document_tokens']:\n",
    "                if not elem['html_token']:\n",
    "                    doc_string.append(elem['token'])\n",
    "            examples['docs'].append(doc_string)\n",
    "            count += 1\n",
    "    file.close()\n",
    "    # CREATING OUR DATASET,\n",
    "    # DOC INDEX = CORRECT QUESTION INDEX, [HARD NEGATIVES INDICES], [EASY NEGATIVES INDICES],\n",
    "\n",
    "    file = open(file_out, 'w+', newline ='\\n')\n",
    "    # file_text = open(text_file_out, 'w')\n",
    "    k_hard = 10\n",
    "    j_rand = 20\n",
    "    bm25 = BM25Okapi(examples['docs'])\n",
    "    print(examples['questions'][0])\n",
    "    for i in range(len(examples['questions'])):\n",
    "        tokenized_query = examples['questions'][i]\n",
    "        doc_scores = bm25.get_scores(tokenized_query)\n",
    "        hard_ix = kmax(k_hard, doc_scores, i)\n",
    "        rand_ix = sample(j_rand, doc_scores, hard_ix + [i])\n",
    "        answers = [[i] + hard_ix + rand_ix]\n",
    "        write = csv.writer(file)\n",
    "        write.writerows(answers)\n",
    "        # file_text.write(' '.join(examples['questions'][i]) + '\\n')\n",
    "        # file_text.write(' '.join(examples['docs'][i]) + '\\n') \n",
    "    file.close()\n",
    "    # file_text.close()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f79d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_vals(elements, tokenizer, encoder):\n",
    "    print('embedding values.')\n",
    "    embeds = []\n",
    "    for step, elem in tqdm(enumerate(elements)):\n",
    "        with torch.no_grad():\n",
    "            tokenized = tokenizer(elem, padding='max_length', max_length = 512,truncation=True )\n",
    "            val = encoder( torch.tensor([tokenized['input_ids']]).to(device) )[0].cpu()\n",
    "            embeds.append(val) \n",
    "        # if step == 10:\n",
    "        #     break\n",
    "    return embeds\n",
    "\n",
    "def get_matrix(embeds):\n",
    "    f = []\n",
    "    for vec in embeds:\n",
    "        s = []\n",
    "        for elem in vec[0]:\n",
    "            s.append(elem)\n",
    "        f.append(s[:])\n",
    "    return torch.tensor(f)\n",
    "\n",
    "\n",
    "def store_vals(file_index, q_embeds, c_embeds, dev):\n",
    "    file_q = get_matrix(q_embeds)\n",
    "    file_c = get_matrix(c_embeds)\n",
    "    if dev:\n",
    "        torch.save(torch.tensor(file_c), f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/dev/context-{file_index}-embeds\")\n",
    "        torch.save(torch.tensor(file_q), f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/dev/question-{file_index}-embeds\")\n",
    "    else:\n",
    "        torch.save(torch.tensor(file_c), f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/train/context-{file_index}-embeds\")\n",
    "        torch.save(torch.tensor(file_q), f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/train/question-{file_index}-embeds\")\n",
    "    \n",
    "def NQ_file_processing(f, file_index, device, dev):\n",
    "    examples = {'docs': [], 'questions': []}\n",
    "    count = 0\n",
    "    cap = 13000 #number of docs per question set in a dataset\n",
    "    #opens file and reads context/question as token vectors\n",
    "    with gzip.open(f, 'r') as file:\n",
    "        for chunk in NQ_file_open(file, device):\n",
    "            if count > cap: \n",
    "                break\n",
    "            if chunk == \"NONE\":\n",
    "                break\n",
    "            examples['questions'].append(chunk['question_text'])\n",
    "            doc_string = \"\"\n",
    "            for elem in chunk['document_tokens']:\n",
    "                if not elem['html_token']:\n",
    "                    doc_string = doc_string + \" \" + elem['token']\n",
    "            examples['docs'].append(doc_string)\n",
    "            count += 1\n",
    "    print(\"FINAL COUNT: \", count)\n",
    "    file.close()\n",
    "    with torch.no_grad():\n",
    "        context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')#.to(device)\n",
    "        context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "        question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')#.to(device)\n",
    "        question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(device)\n",
    "    question_vals = embed_vals(examples['questions'], question_tokenizer, question_encoder)\n",
    "    context_vals = embed_vals(examples['docs'], context_tokenizer, context_encoder)\n",
    "    store_vals(file_index, question_vals, context_vals, dev)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31a4bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE NUMBER:  1\n",
      "['what', 'do', 'the', '3', 'dots', 'mean', 'in', 'math']\n"
     ]
    }
   ],
   "source": [
    "#for dev sets - generating passage rankings\n",
    "for i in range(1, 2):\n",
    "    print(\"FILE NUMBER: \", i)\n",
    "    if i < 10:\n",
    "        file_name = f'/home/ubuntu/nlm/williamyang/Data/NQ/dev/nq-dev-0{i}.jsonl.gz'\n",
    "    else:\n",
    "        file_name = f'/home/ubuntu/nlm/williamyang/Data/NQ/dev/nq-dev-{i}.jsonl.gz'\n",
    "    file_out = f'/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/dev/nq-dev-ix-{i}.csv'\n",
    "    NQ_index_processing(file_name, file_out, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ddf3f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE NUMBER:  0\n",
      "['when', 'is', 'the', 'last', 'episode', 'of', 'season', '8', 'of', 'the', 'walking', 'dead']\n"
     ]
    }
   ],
   "source": [
    "#for train sets - generating passage rankings\n",
    "for i in range(0, 1):\n",
    "    print(\"FILE NUMBER: \", i)\n",
    "    if i < 10:\n",
    "        file_name = f'/home/ubuntu/nlm/williamyang/Data/NQ/train/nq-train-0{i}.jsonl.gz'\n",
    "    else:\n",
    "        file_name = f'/home/ubuntu/nlm/williamyang/Data/NQ/train/nq-train-{i}.jsonl.gz'\n",
    "    file_out = f'/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/train/nq-train-ix-{i}.csv'\n",
    "    NQ_index_processing(file_name, file_out, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eac16bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE NUMBER:  1\n",
      "FINAL COUNT:  1494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1494it [00:20, 74.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1494it [03:59,  6.23it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "#for dev sets - generating context/question embeddings\n",
    "for i in range(1, 2):\n",
    "    print(\"FILE NUMBER: \", i)\n",
    "    if i < 10:\n",
    "        file_name = f'/home/ubuntu/nlm/williamyang/Data/NQ/dev/nq-dev-0{i}.jsonl.gz'\n",
    "    else:\n",
    "        file_name = f'/home/ubuntu/nlm/williamyang/Data/NQ/dev/nq-dev-{i}.jsonl.gz'\n",
    "    NQ_file_processing(file_name, i, device, dev=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861b68d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for train sets - generating context/question embeddings\n",
    "for i in range(0, 2):\n",
    "    print(\"FILE NUMBER: \", i)\n",
    "    if i < 10:\n",
    "        file_name = f'/home/ubuntu/nlm/williamyang/Data/NQ/train/nq-train-0{i}.jsonl.gz'\n",
    "    else:\n",
    "        file_name = f'/home/ubuntu/nlm/williamyang/Data/NQ/train/nq-train-{i}.jsonl.gz'\n",
    "    NQ_file_processing(file_name, i, device, dev=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e86b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing for generating phrase embeddings for testing\n",
    "def NQ_file_open(file_object, device):\n",
    "    while True:\n",
    "        try:\n",
    "            chunk = json.loads(file_object.readline())\n",
    "            yield chunk\n",
    "        except:\n",
    "            yield \"NONE\"\n",
    "\n",
    "def pad_embeds(embeds, target):\n",
    "    x, y = embeds.shape\n",
    "    torch.reshape(embeds, (1, x, y))\n",
    "    proper = torch.clone(target)\n",
    "    proper[:, :x, :] = embeds\n",
    "    return proper\n",
    "\n",
    "def phrase_creator(contexts, phrase_len, batch_size, token_max_len, device):\n",
    "    with torch.no_grad():\n",
    "        context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "        context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "    phrase_embeds = torch.empty((1, 769), dtype=torch.float).to(device)\n",
    "    print(\"TOTAL ITERS:\", int(len(contexts)/batch_size))\n",
    "    for _, i in tqdm(enumerate(range(0, len(contexts), batch_size))):\n",
    "        end = min(i+batch_size, len(contexts))\n",
    "        passage_nums = torch.reshape(torch.tensor([contexts[k][0] for k in range(i, end)]), (end-i, 1)).to(device)\n",
    "        batch = [contexts[j][1] for j in range(i, end)]\n",
    "        with torch.no_grad():\n",
    "            tokenized = context_tokenizer(batch, padding='max_length', max_length = token_max_len,truncation=True)\n",
    "            batch_embeds = context_encoder( torch.tensor(tokenized['input_ids']).to(device) )[0]\n",
    "            final_vals = torch.cat((passage_nums, batch_embeds), 1).to(device)\n",
    "        phrase_embeds = torch.cat((phrase_embeds, final_vals), 0).to(device)\n",
    "    return phrase_embeds[1:, :].cpu()\n",
    "            \n",
    "def store_phrase_vals(file_index, c_embeds, phrase_len, dev):\n",
    "    if dev:\n",
    "        torch.save(torch.tensor(c_embeds), f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/dev/context-{file_index}-{phrase_len}-embeds\")\n",
    "    else:\n",
    "        torch.save(torch.tensor(c_embeds), f\"/home/ubuntu/nlm/williamyang/DPR_Preprocess_Data/train/context-{file_index}-{phrase_len}-embeds\")            \n",
    "            \n",
    "def NQ_phrase_preprocessing(f, file_index, device, phrase_len, dev):\n",
    "    examples = []\n",
    "    count = 0\n",
    "    cap = 13000 #number of docs per question set in a dataset\n",
    "    #opens file and reads context/question as token vectors\n",
    "    with gzip.open(f, 'r') as file:\n",
    "        doc_string = \"\"\n",
    "        for chunk in NQ_file_open(file, device):\n",
    "            ind = 1\n",
    "            if count > cap: \n",
    "                break\n",
    "            if chunk == \"NONE\":\n",
    "                break\n",
    "            for i in range(len(chunk['document_tokens'])):\n",
    "                if not chunk['document_tokens'][i]['html_token']:\n",
    "                    doc_string += \" \" + chunk['document_tokens'][i]['token']\n",
    "                    if ind%phrase_len == 0:\n",
    "                        examples.append((count, doc_string))\n",
    "                        doc_string = \"\"\n",
    "                    ind += 1\n",
    "                if i==len(chunk['document_tokens'])-1:\n",
    "                    examples.append((count, doc_string))\n",
    "                    break\n",
    "            count += 1\n",
    "\n",
    "    file.close()\n",
    "    print(\"COUNT:\", count)\n",
    "    return examples\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e47e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217a0c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COUNT: 5961\n"
     ]
    }
   ],
   "source": [
    "phrase_len = 500\n",
    "dev = False\n",
    "file_index = 0\n",
    "file_path = \"/home/ubuntu/nlm/williamyang/Data/NQ/train/nq-train-00.jsonl.gz\"\n",
    "vals = NQ_phrase_preprocessing(file_path, file_index, device, phrase_len, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52c71c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizer'.\n",
      "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-single-nq-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL ITERS: 4533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4534it [23:31,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "token_max_len = min(phrase_len, 512)\n",
    "embeds = phrase_creator(vals, phrase_len, 20, token_max_len, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf542e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "#for storing values\n",
    "store_phrase_vals(file_index, embeds, phrase_len, dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for clearing values\n",
    "embeds = None\n",
    "vals = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
