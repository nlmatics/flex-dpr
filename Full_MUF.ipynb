{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Documentation**\n",
    "This is a modified version of MUF that can be used for generalization for document retrieval. \n",
    "\n",
    "The only thing that needs to be ensured is that the dataset file you are trying to use is cleaned properly beforehand. Firstly, it needs to be stored in a json file as a dictionary with the following keys: question, text, map. The question key will contain a list of questions, the text key will contain a list of context passages, and the map key will be a dictionary mapping each individual question number to the specific text passages where it's answer is located. See examples of properly formatted dataset files in Noah's folder under the data subfolder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import gzip\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import DPRContextEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoder\n",
    "from transformers import DPRContextEncoder\n",
    "import csv\n",
    "from transformers import BertModel, BertTokenizer, BertTokenizerFast\n",
    "from torch.nn import CosineSimilarity\n",
    "from torch.nn import Softmax\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from nltk import word_tokenize\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from IPython import embed\n",
    "from sklearn.metrics import classification_report\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "torch.cuda.empty_cache()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_phrases(data, token_max_len=256):\n",
    "    '''\n",
    "    Takes as input a list of strings (whether they be questions, contexts etc.) and tokenizes/embeds them\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "        context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "    phrase_embeds = torch.empty((1, 769), dtype=torch.float).to(device)\n",
    "    for i in tqdm(range(len(data))):\n",
    "        passage_num = torch.tensor([[data[i][0]]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            tokenized = context_tokenizer(data[i][1], padding='max_length', max_length = token_max_len,truncation=True)\n",
    "            batch_embeds = context_encoder( torch.tensor([tokenized['input_ids']]).to(device) )[0]\n",
    "            final_val = torch.cat((passage_num, batch_embeds), 1).to(device)\n",
    "        phrase_embeds = torch.cat((phrase_embeds, final_val), 0).to(device)\n",
    "    return phrase_embeds[1:, :].cpu()\n",
    "\n",
    "def embed_contexts(data):\n",
    "    '''Embeds context passages in data'''\n",
    "    with torch.no_grad():\n",
    "        context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')#.to(device)\n",
    "        context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base').to(device)\n",
    "    phrase_embeds = torch.empty((1, 768), dtype=torch.float).to(device)\n",
    "    for i in tqdm(range(len(data))):\n",
    "        with torch.no_grad():\n",
    "            tokenized = context_tokenizer(data[i], padding='max_length', max_length = 512,truncation=True)\n",
    "            batch_embeds = context_encoder( torch.tensor([tokenized['input_ids']]).to(device) )[0]\n",
    "        phrase_embeds = torch.cat((phrase_embeds, batch_embeds), 0).to(device)\n",
    "    return phrase_embeds[1:, :].cpu()\n",
    "\n",
    "def embed_questions(data):\n",
    "    '''Embeds question passages in data'''\n",
    "    with torch.no_grad():\n",
    "        question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')#.to(device)\n",
    "        question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base').to(device)\n",
    "    phrase_embeds = torch.empty((1, 768), dtype=torch.float).to(device)\n",
    "    for i in tqdm(range(len(data))):\n",
    "        with torch.no_grad():\n",
    "            tokenized = question_tokenizer(data[i], padding='max_length', max_length = 512,truncation=True)\n",
    "            batch_embeds = question_encoder( torch.tensor([tokenized['input_ids']]).to(device) )[0]\n",
    "        phrase_embeds = torch.cat((phrase_embeds, batch_embeds), 0).to(device)\n",
    "    return phrase_embeds[1:, :].cpu()\n",
    "\n",
    "def phrase_creator(data, phrase_len):\n",
    "    '''\n",
    "    Given compiled data takes contexts passages and divides them into phrase passages of a given length. Each phrase\n",
    "    is formatted as a tuple, index 0 representing the context passage num it's from, index 1 being the actual phrase\n",
    "    '''\n",
    "    punc = ['.', '!', '?']\n",
    "    phrases = []\n",
    "    for i in range(len(data)):\n",
    "        context_sents = data[i]\n",
    "        batch = \"\"\n",
    "        punc_count = 0\n",
    "        punc_diff = 0\n",
    "        for j in range(len(context_sents)):\n",
    "            if context_sents[j] in ['.', '!', '?']:\n",
    "                if punc_diff > 120: \n",
    "                    punc_count += 1\n",
    "                if punc_count >= phrase_len:\n",
    "                    punc_count = 0\n",
    "                    phrases.append((i, batch))\n",
    "                    batch = \"\"\n",
    "                punc_diff = 0\n",
    "                continue\n",
    "            batch = batch + context_sents[j]\n",
    "            punc_diff += 1\n",
    "        phrases.append((i, batch))\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL DATASET CLASSES USED in MUF, there are distinct ones for DPR and Dense Phrase Retrieval \n",
    "class DPR_Dataset(Dataset):\n",
    "    def __init__(self, context_file, question_file, idxs, NQ, map):\n",
    "        self.context_embeds = context_file\n",
    "        self.question_embeds = question_file\n",
    "        self.context_embeds = []\n",
    "        self.idxs = idxs\n",
    "        if not NQ:\n",
    "            self.map = map\n",
    "        else:\n",
    "            self.map = {}\n",
    "            for i in range(len(idxs)):\n",
    "                self.map[str(i)] = i\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_ixs = self.idxs[idx]\n",
    "        query_embed = self.question_embeds[idx ]\n",
    "        context_embeds = [self.context_embeds[int(batch_ixs[i])]  for i in range(len(batch_ixs))  ]\n",
    "        label = batch_ixs[0]\n",
    "        return (query_embed, context_embeds, batch_ixs[:], label)\n",
    "        \n",
    "class Phrase_Dataset(Dataset):\n",
    "    def __init__(self, context_file, question_file, idxs, NQ, map):\n",
    "        self.context_embeds = context_file\n",
    "        self.question_embeds = question_file\n",
    "        self.context_embeds = {} #define context vals as dict with keys as passage nums and values in entries as passage encodings\n",
    "        self.idxs = idxs\n",
    "        if not NQ:\n",
    "            self.map = map\n",
    "        else:\n",
    "            self.map = {}\n",
    "            for i in range(len(idxs)):\n",
    "                self.map[str(i)] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_ixs = self.idxs[idx]\n",
    "        query_embed = self.question_embeds[idx]\n",
    "        context_embeds = []\n",
    "        for i in range(len(batch_ixs)):\n",
    "            ind = batch_ixs[i]\n",
    "            for elem in self.context_embeds[int(ind)]:\n",
    "                context_embeds.append(elem)\n",
    "        label = self.map[str(idx)]\n",
    "        return (query_embed, context_embeds, batch_ixs[:], label)\n",
    "\n",
    "    def collate_fn_(self, data):\n",
    "        q_batch = [q[0] for q in data]\n",
    "        c_batch = [c[1] for c in data]\n",
    "        ixs_batch = [i[2] for i in data]\n",
    "        label_batch = [l[3] for l in data]\n",
    "        max_len = max([len(c_batch[j]) for j in range(len(c_batch))])\n",
    "        default = torch.full((1, 769), 0)\n",
    "        for elem in c_batch:\n",
    "            while len(elem) < max_len:\n",
    "                elem.append(default)\n",
    "        return q_batch, c_batch, ixs_batch, label_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_len = 10\n",
    "def conf_linear(similarities):\n",
    "    if len(similarities) < test_len:\n",
    "        elems = np.sort(similarities)\n",
    "    else:\n",
    "        elems = np.sort(similarities)[-test_len:]\n",
    "    conf = 0\n",
    "    for i in range(-2, -test_len, -1):\n",
    "        conf += elems[-1]-elems[i]\n",
    "    return conf/8\n",
    "\n",
    "def softmax_func(elems):\n",
    "    return np.exp(elems)/np.sum(np.exp(elems))\n",
    "\n",
    "def softmax_grad(first, m, elems, T):\n",
    "    '''\n",
    "    first variable represents the value in the numerator of the softmax function\n",
    "    m is the exponent of the first variable\n",
    "    elems are the original logit values\n",
    "    '''\n",
    "    grad = 0\n",
    "    div_elems = elems/T\n",
    "    p = np.exp(div_elems)\n",
    "    den = np.sum(p)\n",
    "    s1 = 0\n",
    "    for i in range(len(elems)):\n",
    "        s1 += elems[i]*p[i]\n",
    "    s1 *= first/(T*den)**2\n",
    "    s2 = m*first/((T**2)*den)\n",
    "    return s1-s2\n",
    "\n",
    "def conf_softmax(similarities, T=1):\n",
    "    '''\n",
    "    This confidence measure includes calirbation of the logits with temperature scaling\n",
    "    '''\n",
    "    if len(similarities) < test_len:\n",
    "        elems = np.sort(similarities)\n",
    "    else:\n",
    "        elems = np.sort(similarities)[-test_len:]\n",
    "    div_elems = elems/T\n",
    "    calb_vals = softmax_func(div_elems)\n",
    "    grad = 0\n",
    "    conf = calb_vals[-1]\n",
    "    grad = softmax_grad(calb_vals[-1], elems[-1], elems[:], T)\n",
    "    # for i in range(-2, -test_len, -1):\n",
    "    #     conf += calb_vals[-1]-calb_vals[i]\n",
    "    #     grad += softmax_grad(calb_vals[-1], elems[-1], elems[:], T)-softmax_grad(calb_vals[i], elems[i], elems, T)\n",
    "    return conf, grad\n",
    "\n",
    "def run_DensePhraseRet(query_embed, context_embeds, ixs, confidence_func, T=1):\n",
    "    similarities = []\n",
    "    for i in range(len(context_embeds)):\n",
    "        context_val = context_embeds[i][0][1:]\n",
    "        similarities.append( (torch.dot(query_embed[0], context_val)/torch.norm(context_val)).to(float).cpu())\n",
    "    similarities = np.array(similarities)\n",
    "    similarities = np.where(np.isnan(similarities), 0, similarities)\n",
    "    return int(context_embeds[np.argmax(np.array(similarities))][0][0]), confidence_func(similarities, T)\n",
    "\n",
    "def run_DPR(query_embed, context_embeds, ixs, confidence_func, T=1):\n",
    "    similarities = []\n",
    "    for i in range(len(context_embeds)):\n",
    "        context_val = context_embeds[i][0]\n",
    "        similarities.append( (torch.dot(query_embed[0], context_val)/torch.norm(context_val )).to(float).cpu())\n",
    "    similarities = np.array(similarities)\n",
    "    return int(ixs[np.argmax(similarities)]), confidence_func(similarities, T)\n",
    "\n",
    "\n",
    "def update_batch(preds, batch_acc, label):\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == label:\n",
    "            batch_acc.append(1)\n",
    "        else:\n",
    "            batch_acc.append(0)\n",
    "\n",
    "def update_bins(bins, preds, confs, increments, label):\n",
    "    for j in range(len(confs)):\n",
    "        c = confs[j][0]\n",
    "        for i in range(1, increments+1):\n",
    "            if c <= i*1/increments:\n",
    "                bins[i*1/increments]['grads'].append(confs[j][1])\n",
    "                bins[i*1/increments]['confs'].append(c)\n",
    "                if preds[j] == label: bins[i*1/increments]['acc'] += 1\n",
    "                break\n",
    "\n",
    "def calc_bin_loss(bins):\n",
    "    loss = 0\n",
    "    grad = 0\n",
    "    N = 0\n",
    "    for k in bins.keys():\n",
    "        N += len(bins[k]['confs'])\n",
    "    for t in bins.keys():\n",
    "        if len(bins[t]['confs']) == 0: continue\n",
    "        avg_conf = sum(bins[t]['confs'])/len(bins[t]['confs'])\n",
    "        avg_grad = sum(bins[t]['grads'])/len(bins[t]['grads'])\n",
    "        acc = bins[t]['acc']/len(bins[t]['confs'])\n",
    "        grad += len(bins[t]['confs'])/N*(avg_conf-acc)*avg_grad\n",
    "        loss += len(bins[t]['confs'])/N*(avg_conf-acc)**2\n",
    "    return loss, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MUF Evaluation Function\n",
    "#REGULAR NO BATCH MUF\n",
    "def eval(T_init, rows, datasets):\n",
    "    phrase_lens = [0, 1, 3, 5]\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    batch_conf = []\n",
    "    batch_acc = []\n",
    "    conf_func = conf_softmax\n",
    "    temp = T_init\n",
    "    step = 100\n",
    "    increments = 10\n",
    "    bins = {i/increments: {\"confs\": [], \"acc\": 0, \"grads\": []} for i in range(1, increments+1)}\n",
    "    for i in tqdm(range(len(rows))):\n",
    "        # if i == 2: break\n",
    "        preds = []\n",
    "        for d in range(4):\n",
    "            query, context, ixs, label = datasets[d][i]\n",
    "            if phrase_lens[d] == 0:\n",
    "                preds.append(run_DPR(query, context, ixs, conf_func, temp))\n",
    "            else:\n",
    "                preds.append(run_DensePhraseRet(query, context, ixs, conf_func, temp))\n",
    "        confs = np.array([elem[1][0] for elem in preds])\n",
    "        conf_grads = np.array([en[1][1] for en in preds])\n",
    "        np.where(np.isnan(conf_grads), 0, conf_grads)\n",
    "        np.where(np.isnan(confs), 0, confs)\n",
    "        cum_data = np.array(list(zip(confs, conf_grads)))\n",
    "        confs = list(confs)\n",
    "        pred = preds[confs.index(max(confs))][0]\n",
    "        update_bins(bins, preds, cum_data, increments, label)\n",
    "        y_pred.append(int(pred))\n",
    "        y_label.append(int(label))\n",
    "        if i%int(len(rows)/10) == 0 and i != 0:\n",
    "            loss, grad = calc_bin_loss(bins)\n",
    "            temp += float(step*grad)\n",
    "            bins = {i/increments: {\"confs\": [], \"acc\": 0, \"grads\": []} for i in range(1, increments+1)}\n",
    "\n",
    "    return classification_report(y_label, y_pred, digits=4, output_dict=True)['macro avg']['f1-score']\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating Question, Context, Phrase Embeds, then generates dataset\n",
    "\n",
    "data_file = \"\" #CHANGE THIS TO FILE OF DATA YOU WANT TO PROCESS\n",
    "with open(data_file) as file: \n",
    "    data = json.load(file)\n",
    "context_embeds = embed_contexts(data['text']) \n",
    "question_embeds = embed_questions(data['question'])\n",
    "phrases = [phrase_creator(data['text'], i) for i in [1, 3, 5]]\n",
    "phrase_embeds = [embed_phrases(elem) for elem in phrases]\n",
    "idxs = []\n",
    "m = [i for i in range(len(data['question']))]\n",
    "for k in range(len(data['question'])):\n",
    "    z = m[:]\n",
    "    z[k] = 0\n",
    "    z[0] = k\n",
    "    idxs.append(z)\n",
    "\n",
    "datasets = []\n",
    "for j in range(4):\n",
    "    if j == 0:\n",
    "        datasets.append(DPR_Dataset(context_embeds, question_embeds, idxs, data['map']))\n",
    "    else:\n",
    "        datasets.append(Phrase_Dataset(phrase_embeds[j-1], question_embeds, idxs, data['map']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval(0.1, idxs, datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
